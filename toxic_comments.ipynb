{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "collapsed": false,
        "id": "QkfVgS3QypqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-b-oIVA3Qmh",
        "outputId": "9a23c5e7-5a6c-48e4-9f0d-48405b9f8255"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.10/dist-packages (1.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "start_time": "2023-04-29T04:06:40.075330Z",
          "end_time": "2023-04-29T04:06:45.815758Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gMp2u2KypqK",
        "outputId": "d9ca0263-731b-4dc1-c41c-22a6c0f878d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import CuDNNLSTM, CuDNNGRU\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import optimizers\n",
        "from keras.layers import Lambda\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "\n",
        "import gc\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from unidecode import unidecode\n",
        "\n",
        "import time\n",
        "\n",
        "eng_stopwords = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "We have severals step in preprocessing:  \n",
        "1) cleain_text: delete some useless charactes like digits and punctuation marks.  \n",
        "2) revise_cuts: delete cuts like \"you're\" or \"i'm\" and so on.  \n",
        "3) revise_star: due to stars often used for disguise bad words, we create set of the most popular bad words and check: \"does the word in bad if delete all start?\", if yes then delete stars.  \n",
        "4) revise_triple_and_more_letters: delete char duplicates.  \n",
        "5) revise_redundancy_words: delete redundancy of bad words."
      ],
      "metadata": {
        "collapsed": false,
        "id": "YUBN7HqyypqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
        "\n",
        "COMMENT_COL = 'comment_text'\n",
        "ID_COL = 'id'\n",
        "input_dir = './data/'\n",
        "output_dir = './output/'\n",
        "embeddings_dir = \"./drive/MyDrive/embeddings/\""
      ],
      "metadata": {
        "id": "89ozgbX62VtD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuts = {\n",
        "    \"you're\": ['you', 'are'],\n",
        "    \"i'm\": ['i', 'am'],\n",
        "    \"he's\": ['he', 'is'],\n",
        "    \"she's\": ['she', 'is'],\n",
        "    \"it's\": ['it', 'is'],\n",
        "    \"they're\": ['they', 'are'],\n",
        "    \"can't\": ['can', 'not'],\n",
        "    \"couldn't\": ['could', 'not'],\n",
        "    \"don't\": ['do', 'not'],\n",
        "    \"don;t\": ['do', 'not'],\n",
        "    \"didn't\": ['did', 'not'],\n",
        "    \"doesn't\": ['does', 'not'],\n",
        "    \"isn't\": ['is', 'not'],\n",
        "    \"wasn't\": ['was', 'not'],\n",
        "    \"aren't\": ['are', 'not'],\n",
        "    \"weren't\": ['were', 'not'],\n",
        "    \"won't\": ['will', 'not'],\n",
        "    \"wouldn't\": ['would', 'not'],\n",
        "    \"hasn't\": ['has', 'not'],\n",
        "    \"haven't\": ['have', 'not'],\n",
        "    \"what's\": ['what', 'is'],\n",
        "    \"that's\": ['that', 'is'],\n",
        "}\n",
        "set_cuts = set(cuts.keys())"
      ],
      "metadata": {
        "id": "zw5mR2A70ZJc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_indicator_words = [\n",
        "    'fuck', 'fucking', 'fucked', 'fuckin', 'fucka', 'fucker', 'fucks', 'fuckers',\n",
        "    'fck', 'fcking', 'fcked', 'fckin', 'fcker', 'fcks',\n",
        "    'fuk', 'fuking', 'fuked', 'fukin', 'fuker', 'fuks', 'fukers',\n",
        "    'fk', 'fking', 'fked', 'fkin', 'fker', 'fks',\n",
        "    'shit', 'shitty', 'shite',\n",
        "    'stupid', 'stupids',\n",
        "    'idiot', 'idiots',\n",
        "    'suck', 'sucker', 'sucks', 'sucka', 'sucked', 'sucking',\n",
        "    'ass', 'asses', 'asshole', 'assholes', 'ashole', 'asholes',\n",
        "    'gay', 'gays',\n",
        "    'niga', 'nigga', 'nigar', 'niggar', 'niger', 'nigger',\n",
        "    'monster', 'monsters',\n",
        "    'loser', 'losers',\n",
        "    'nazi', 'nazis',\n",
        "    'cock', 'cocks', 'cocker', 'cockers',\n",
        "    'faggot', 'faggy',\n",
        "]\n",
        "toxic_indicator_words_sets = set(toxic_indicator_words)"
      ],
      "metadata": {
        "id": "fQd0bYuC2bKb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "redundancy_rightFormat = {\n",
        "    'ckckck': 'cock',\n",
        "    'fuckfuck': 'fuck',\n",
        "    'lolol': 'lol',\n",
        "    'lollol': 'lol',\n",
        "    'pussyfuck':'fuck',\n",
        "    'gaygay': 'gay',\n",
        "    'haha': 'ha',\n",
        "    'sucksuck': 'suck'}\n",
        "\n",
        "redundancy = set(redundancy_rightFormat.keys())"
      ],
      "metadata": {
        "id": "Wa49NSrL28pC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CommProcess(object):\n",
        "    @staticmethod\n",
        "    def clean_text(t):\n",
        "        t = re.sub(r\"[^A-Za-z0-9,!?*.;’´'\\/]\", \" \", t)\n",
        "        t = replace_numbers.sub(\" \", t)\n",
        "        t = t.lower()\n",
        "        t = re.sub(r\",\", \" \", t)\n",
        "        t = re.sub(r\"’\", \"'\", t)\n",
        "        t = re.sub(r\"´\", \"'\", t)\n",
        "        t = re.sub(r\"\\.\", \" \", t)\n",
        "        t = re.sub(r\"!\", \" ! \", t)\n",
        "        t = re.sub(r\"\\?\", \" ? \", t)\n",
        "        t = re.sub(r\"\\/\", \" \", t)\n",
        "        return t\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_cuts(t):\n",
        "        ret = []\n",
        "        for word in t.split():\n",
        "            if word in set_cuts:\n",
        "                ret.append(cuts[word][0])\n",
        "                ret.append(cuts[word][1])\n",
        "            else:\n",
        "                ret.append(word)\n",
        "        ret = ' '.join(ret)\n",
        "        ret = re.sub(\"'\", \" \", ret)\n",
        "        ret = re.sub(r\";\", \" \", ret)\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_star(t):\n",
        "        ret = []\n",
        "        for word in t.split():\n",
        "            if ('*' in word) and (re.sub('\\*', '', word) in toxic_indicator_words_sets):\n",
        "                word = re.sub('\\*', '', word)\n",
        "            ret.append(word)\n",
        "        ret = re.sub('\\*', ' ', ' '.join(ret))\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_triple_and_more_letters(t):\n",
        "        for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "            reg = letter + \"{2,}\"\n",
        "            t = re.sub(reg, letter + letter, t)\n",
        "        return t\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_redundancy_words(t):\n",
        "        ret = []\n",
        "        for word in t.split(' '):\n",
        "            for redu in redundancy:\n",
        "                if redu in word:\n",
        "                    word = redundancy_rightFormat[redu]\n",
        "                    break\n",
        "            ret.append(word)\n",
        "        return ' '.join(ret)\n",
        "\n",
        "\n",
        "def execute_comm_process(df):\n",
        "    comm_process_pipeline = [\n",
        "        CommProcess.clean_text,\n",
        "        CommProcess.revise_cuts,\n",
        "        CommProcess.revise_star,\n",
        "        CommProcess.revise_triple_and_more_letters,\n",
        "        CommProcess.revise_redundancy_words,\n",
        "    ]\n",
        "    for cp in comm_process_pipeline:\n",
        "        df[COMMENT_COL] = df[COMMENT_COL].apply(cp)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "R4-8F_3N00er"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read files\n",
        "Read files and save 'processed' data train and test files using previous preprocessing."
      ],
      "metadata": {
        "id": "uGUxh2593joq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(input_dir + 'train.csv')\n",
        "df_test = pd.read_csv(input_dir + 'test.csv')"
      ],
      "metadata": {
        "id": "il5MY3dU6rUF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = execute_comm_process(df_train)\n",
        "df_train.to_csv(input_dir + 'train_processed.csv', index=False)\n",
        "\n",
        "df_test = execute_comm_process(df_test)\n",
        "df_test.to_csv(input_dir+ 'test_processed.csv', index=False)\n"
      ],
      "metadata": {
        "id": "5K90O_n43hms"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(input_dir + 'train_processed.csv')\n",
        "test = pd.read_csv(input_dir + 'test_processed.csv')"
      ],
      "metadata": {
        "id": "1mEyt6zY8zBp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove non-ascii characters"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ay4ausl6ypqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n",
        "def clean_text(x):\n",
        "    x_ascii = unidecode(x)\n",
        "    x_clean = special_character_removal.sub('',x_ascii)\n",
        "    return x_clean\n",
        "\n",
        "train['comment_text'] = train['comment_text'].apply(lambda x: clean_text(str(x)))\n",
        "test['comment_text'] = test['comment_text'].apply(lambda x: clean_text(str(x)))\n",
        "\n",
        "X_train = train['comment_text'].fillna(\"something\").values\n",
        "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
        "X_test = test['comment_text'].fillna(\"something\").values"
      ],
      "metadata": {
        "id": "zcZwE76wypqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add features and scale data\n",
        "New features will be used in the process of training and it can increase accuracy/score."
      ],
      "metadata": {
        "collapsed": false,
        "id": "_4XvwAxnypqN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": [
        "def add_features(df):\n",
        "    df['comment_text'] = df['comment_text'].apply(str)\n",
        "    df['total_length'] = df['comment_text'].apply(len)\n",
        "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
        "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']), axis=1)\n",
        "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
        "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
        "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
        "\n",
        "    return df\n",
        "\n",
        "# add features to dataset\n",
        "train = add_features(train)\n",
        "test = add_features(test)\n",
        "\n",
        "# get features from dataset\n",
        "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
        "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
        "\n",
        "# Initialize StandardScaler and scale features\n",
        "ss = StandardScaler()\n",
        "ss.fit(np.vstack((features, test_features)))\n",
        "features = ss.transform(features)\n",
        "test_features = ss.transform(test_features)"
      ],
      "metadata": {
        "id": "-yurTjm2ypqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize data"
      ],
      "metadata": {
        "collapsed": false,
        "id": "w_Hox_2eypqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "283759\n"
          ]
        }
      ],
      "source": [
        "max_features = 300000\n",
        "maxlen = 900\n",
        "\n",
        "# Initialize tokenizer and tokenize dataset\n",
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
        "X_train_sequence = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequence = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Add pads into tokenized data to all sentences will be the same length\n",
        "x_train = pad_sequences(X_train_sequence, maxlen=maxlen)\n",
        "x_test = pad_sequences(X_test_sequence, maxlen=maxlen)\n",
        "print(len(tokenizer.word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC9M0wqkypqP",
        "outputId": "a4b6b3b7-212e-4a49-a957-361ee7652ef2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Embeddings"
      ],
      "metadata": {
        "id": "h-US57QAMxKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": [
        "# Load the FastText Web Crawl vectors\n",
        "EMBEDDING_FILE_FASTTEXT = embeddings_dir + \"fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
        "EMBEDDING_FILE_TWITTER = embeddings_dir + \"glove-twitter-27b-200d-txt/glove.twitter.27B.200d.txt\"\n",
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\n",
        "embeddings_index_tw = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))"
      ],
      "metadata": {
        "id": "kAldqmspypqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spelling correction\n",
        "We used spelling correction which was used in the 2-nd assignment. It will used in the embedding step. We use spell correction in the situation when embedding can embed the word, so we assume that in the word can be some mistake or typo, we fix it using spell correction and try to use embedding again, but already on the correct word."
      ],
      "metadata": {
        "collapsed": false,
        "id": "ZNhyKkQ4ypqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the 2-nd assignment we used two boooks. Here we can use vocabulary from FASTTEXT."
      ],
      "metadata": {
        "id": "oZx6g1Q4MmOc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": [
        "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)"
      ],
      "metadata": {
        "id": "MzlcMlKVypqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": [
        "words = spell_model.index_to_key\n",
        "\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "\n",
        "WORDS = w_rank\n",
        "\n",
        "# Use fast text as vocabulary\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def P(word):\n",
        "    \"\"\"Probability of `word`.\"\"\"\n",
        "    # use inverse of rank as proxy\n",
        "    # returns 0 if the word isn't in the dictionary\n",
        "    return - WORDS.get(word, 0)\n",
        "\n",
        "def correction(word):\n",
        "    \"\"\"Most probable spelling correction for word.\"\"\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"\"\"Generate possible spelling corrections for word.\"\"\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"\"\"The subset of `words` that appear in the dictionary of WORDS.\"\"\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"\"\"All edits that are one edit away from `word`.\"\"\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"\"\"All edits that are two edits away from `word`.\"\"\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def singlify(word):\n",
        "    return \"\".join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i-1]])\n"
      ],
      "metadata": {
        "id": "_vfzyuuyypqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embed words\n",
        "Size of embeded vector is 501. And to get embeded vector we use tasttext and twitter embedding. First, we create embeded vector for word 'something' because in the preprocessing step we changed NaN to word 'something' so it better to have it. Then we generate embeded vectors for all tokenized words. We make vector where [:300,] are from fasttext and [300:500,] from twitter ebmbedding. Last 501-th number is show: does the word is the first word in the sentences or not(it's check the first letter of the word). If we can't get embedding vector from word, even with spell corecctions, then we make it like word \"something\" vector."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Q7VllbGUypqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": [
        "word_index = tokenizer.word_index\n",
        "number_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((number_words,501))\n",
        "\n",
        "something_tw = embeddings_index_tw.get(\"something\")\n",
        "something_ft = embeddings_index_ft.get(\"something\")\n",
        "\n",
        "something = np.zeros((501,))\n",
        "something[:300,] = something_ft\n",
        "something[300:500,] = something_tw\n",
        "something[500,] = 0\n",
        "\n",
        "def all_caps(word):\n",
        "    return len(word) > 1 and word.isupper()\n",
        "\n",
        "def embed_word(embedding_matrix, i, word):\n",
        "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
        "    if embedding_vector_ft is not None:\n",
        "        if all_caps(word):\n",
        "            last_value = np.array([1])\n",
        "        else:\n",
        "            last_value = np.array([0])\n",
        "        embedding_matrix[i,:300] = embedding_vector_ft\n",
        "        embedding_matrix[i,500] = last_value\n",
        "        embedding_vector_tw = embeddings_index_tw.get(word)\n",
        "        if embedding_vector_tw is not None:\n",
        "            embedding_matrix[i,300:500] = embedding_vector_tw\n",
        "\n",
        "# Fasttext vector is used by itself if there is no glove vector but not the other way around.\n",
        "for word, i in word_index.items():\n",
        "    if i - 1 >= max_features: continue\n",
        "\n",
        "    if embeddings_index_ft.get(word) is not None:\n",
        "        embed_word(embedding_matrix,i-1,word)\n",
        "    else:\n",
        "        # change to > 20 for better score.\n",
        "        if len(word) > 0:\n",
        "            embedding_matrix[i-1] = something\n",
        "        else:\n",
        "            word2 = correction(word)\n",
        "            if embeddings_index_ft.get(word2) is not None:\n",
        "                embed_word(embedding_matrix,i-1,word2)\n",
        "            else:\n",
        "                word2 = correction(singlify(word))\n",
        "                if embeddings_index_ft.get(word2) is not None:\n",
        "                    embed_word(embedding_matrix,i-1,word2)\n",
        "                else:\n",
        "                    embedding_matrix[i-1] = something"
      ],
      "metadata": {
        "id": "Dli4-SsWypqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "Roc Auc Score which is used in kaggle compition to calculate test score."
      ],
      "metadata": {
        "collapsed": false,
        "id": "xvVav_W8ypqR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        self.max_score = 0\n",
        "        self.not_better_count = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
        "            if (score > self.max_score):\n",
        "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
        "                model.save_weights(\"best_weights.h5\")\n",
        "                self.max_score=score\n",
        "                self.not_better_count = 0\n",
        "            else:\n",
        "                self.not_better_count += 1\n",
        "                if self.not_better_count > 3:\n",
        "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
        "                    self.model.stop_training = True"
      ],
      "metadata": {
        "id": "0A3KwGU-ypqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model description\n",
        "First layer: As We said latter: concatenated fasttext and glove twitter embeddings. Fasttext vector is used by itself if there is no glove vector but not the other way around. Words without word vectors are replaced with a word vector for a word \"something\". Also, We added additional value that was set to 1 if a word was written in all capital letters and 0 otherwise.\n",
        "\n",
        "Second layer: SpatialDropout1D(0.5)\n",
        "\n",
        "Third layer: Bidirectional CuDNNLSTM with a kernel size 40. We found out that LSTM as a first layer works better than GRU.\n",
        "\n",
        "Fourth layer: Bidirectional CuDNNGRU with a kernel size 40.\n",
        "\n",
        "Fifth layer: A concatenation of the last state, maximum pool, average pool and two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
        "\n",
        "Sixth layer: output dense layer."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Sp1d5xR1ypqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": [
        "def get_model(features, clipvalue=1., num_filters=40, dropout=0.5, embed_size=501):\n",
        "    features_input = Input(shape=(features.shape[1],))\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "\n",
        "    # Layer 1: concatenated fasttext and glove twitter embeddings.\n",
        "    x = Embedding(number_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "\n",
        "    # Layer 2: SpatialDropout1D(0.5)\n",
        "    x = SpatialDropout1D(dropout)(x)\n",
        "\n",
        "    # Layer 3: Bidirectional CuDNNLSTM\n",
        "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
        "\n",
        "    # Layer 4: Bidirectional CuDNNGRU\n",
        "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)\n",
        "\n",
        "    # Layer 5: A concatenation of the last state, maximum pool, average pool and\n",
        "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "\n",
        "    x = concatenate([avg_pool, x_h, max_pool,features_input])\n",
        "\n",
        "    # Layer 6: output dense layer.\n",
        "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=[inp,features_input], outputs=outp)\n",
        "    adam = optimizers.adam(clipvalue=clipvalue)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "cfDpA09oypqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "collapsed": false,
        "id": "KdsYKQwmypqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(283759, 501)\n"
          ]
        }
      ],
      "source": [
        "model = get_model(features)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "epochs = 10\n",
        "gc.collect()\n",
        "K.clear_session()\n",
        "\n",
        "num_folds = 10 \n",
        "\n",
        "predict = np.zeros((test.shape[0],6))\n",
        "\n",
        "scores = []\n",
        "oof_predict = np.zeros((train.shape[0],6))\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n",
        "\n",
        "for train_index, test_index in KFold(n_splits=10, shuffle=True, random_state=239).split(x_train):\n",
        "    x_train_new = x_train[train_index]\n",
        "    features_train_new = features[train_index]\n",
        "    y_train_new = y_train[train_index]\n",
        "\n",
        "    x_test_new = x_train[test_index]\n",
        "    features_test_new = features[test_index]\n",
        "    y_test_new = y_train[test_index]\n",
        "    \n",
        "    for _train_index, val_index in kf.split(x_train_new):\n",
        "        kfold_X_train = x_train_new[_train_index]\n",
        "        kfold_X_features = features_train_new[_train_index]\n",
        "        kfold_y_train = y_train_new[_train_index]\n",
        "\n",
        "        kfold_X_val = x_train_new[val_index]\n",
        "        kfold_X_val_features = features_train_new[val_index]\n",
        "        kfold_y_val = y_train_new[val_index]\n",
        "\n",
        "        gc.collect()\n",
        "        K.clear_session()\n",
        "\n",
        "        model = get_model(features)\n",
        "\n",
        "        ra_val = RocAucEvaluation(validation_data=([kfold_X_val,kfold_X_val_features], kfold_y_val), interval = 1)\n",
        "\n",
        "        model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "                callbacks = [ra_val])\n",
        "        gc.collect()\n",
        "        model.load_weights(\"best_weights.h5\")\n",
        "\n",
        "    predict = model.predict([x_test_new,features_test_new], batch_size=batch_size,verbose=1)\n",
        "    score = roc_auc_score(y_test_new, predict)\n",
        "    print(\"Test core:\", score)\n",
        "    break\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH_Gy44WypqS",
        "outputId": "67eacd72-1caf-4413-c39c-5560ded98364"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}